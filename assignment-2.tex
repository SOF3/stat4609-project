% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{csquotes}
\usepackage{mathtools}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{2021}
%\setcounter{page}{4321} % For final version only

\newcommand{\q}[1]{\enquote{#1}}
\newcommand{\Set}[1]{\left\{ #1 \right\}}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\begin{document}

%%%%%%%%% TITLE
\title{
	Assignment 2 - KNN Recommendation System \\~\\
	\large{Team name: M2 Robo}
}

\author{
	Chan Kwan Yin\\
	3035466978 \\
	Team leader

	\and

	Lee Chun Yin\\
	3035469140\\

	\and

	Chiu Yu Ying\\
	3035477630
}

\maketitle

\clearpage

\section{Background}

In the KNN ($k$-nearest neighbours) model, we assume that similar users will give close ratings on similar movies.
To predict the rating $\hat r_{ij}$ of user $i$ on movie $j$ ($1 \le i \le m, 2 \le j \le n$),
the $k$ most similar users ($n_1, \ldots, n_k$) to user $i$ are computed based on similar choices of ratings,
and $\hat r_{ij}$ is estimated based on the known values $r_{n_1j}, \ldots, r_{n_kj}$.

\subsection{Similarity (Nearness) metrics}
The similarity metric $d : \mathbb R^n \times \mathbb R^n \to \mathbb R$ used in the KNN model satisfies
\begin{equation} \label{eq:metric} \begin{cases}
	d(x, y) = 0 &\iff x = y \\
	0 < d(x, y) \leq 1 &\iff x \ne y \\
	d(x, y) = d(y, x) &\forall x, y
\end{cases} \end{equation}

While $d(i, j) < d(i, k)$ implies that $j$ is more similar to $i$ than $k$ is,
it is not necessary that the metric follows the triangular inequality.
This enables the use of correlation coefficients in addition to common norm metrics.

\subsubsection{Pearson correlation}
Considering a user $u_i$ rates movies with a distribution $R_i \sim (\mu_i,\sigma_i)$,
the correlation between user $u_i$ and the other user $u_j$
would be the coefficient between the two distributions $R_i$ and $R_j$:
$$ \rho_{ij} = \frac{E[(R_i - \mu_i)(R_j - \mu_j)]}{\sigma_i \sigma_j} $$

To get the $k$ similar data by considering the $n$ movies that both user $u_i$ and $u_j$ have, we estimate the co-variance and variances:
$$ E[(R_i-\mu_i)(R_j-\mu_j)] \approx \frac{1}{n} \sum_k (r_{ik} - \mu_i) (r_{jk} - \mu_j)$$
$$ \sigma_i \approx \sqrt{\frac{1}{n} \sum_k (r_{ik} - \mu_i)} ,
 \sigma_j \approx \sqrt{\frac{1}{n} \sum_k (r_{jk} - \mu_i) ,}$$

Note that the range of the Pearson Correlation Coefficient is $[-1,1]$.
To make the coefficient satisfy the conditions in~\ref{eq:metric}
and not have negative values, we define $$ d(i, j) = 1 - \rho_{ij} $$
such that $d$ has the range $[0, 2]$.
A smaller value implies higher similarity between users.

\subsubsection{Spearman Correlation}
Contrary to Pearson's choice of mean and variance,
Spearman Correlation uses the ranking of variables in the vector as the metric
to eliminate effects of non-uniform distributions.

$$ \rho_{ij} = \frac{
	\operatorname{Cov}({\operatorname{rank}_i, \operatorname{rank}_j})
}{
	\sigma_{\operatorname{rank}_i} \sigma_{\operatorname{rank}_j}
} $$

Similar to Pearson Correlation, the metric based on Spearman correlation is defined as
$$ d(i, j) = 1 - \rho_{ij} $$

\subsubsection{Euclidean Distance}
We take the $2$-norm squared:

$$ d(i, j) = \sum_{l=1}^n \left( r_{il} - r_{jl} \right)^2 $$

\subsubsection{Taxicab as a similarity metric}
We take the $1$-norm:

$$ d(i, j) = \sum_{l=1}^n \left| r_{il} - r_{jl} \right| $$

\subsubsection{Cosine similarity}
This can be thought as the cosine of the angle between the two vectors of ratings.
We define

$$ d(i, j) = \frac1{ \norm{\tilde{\mathbf r}_{i\cdot}} 
+ \norm{\tilde{\mathbf r}_{j\cdot}} }
\sum_{l=1}^n \left( \tilde r_{il} \tilde r_{jl} \right) $$

This metric is sensitive to the exact value (rather than just relative difference);
a negative value could imply that the user does not like the data.
Therefore, we propose two mechanisms for rescaling the ratings,
using $\tilde r_{ij} = r_{ij} - \bar{\mathbf r_{\cdot\cdot}}$
and $\tilde r_{ij} = r_{ij} - 3$ respectively,
where $3$ is the standard rating for "neutral".

\subsection{Aggregation of ratings}
After the $k$ nearest neighbour users have been selected,
$\hat r_{ij}$ can be estimated by aggregating $\Set{ r_{n_1j}, r_{n_2j}, \ldots, r_{n_kj} }$.

A naive aggregation method is to just take the arithmetic mean of these rating values
without considering other factors such as the actual correlation.

Considering the case when all the similar users do not have a rating of the movie, the rating may be high due to the other less similar users. We believed that implementing the weights based on the similarity is important:

$$ \hat r_{uj} = \mu_u + \frac{\sum_{v \in P_u(j)}  Sim(u,v)\cdot(r_{vj}-\mu_v)}{\sum_{v \in P_u(j) |Sim(u,v)|}}  $$

Following this formula, the more similar user's rating will have higher weight.

\section{Technical Details}
The training data set provided by Netflix consists of more than 100 million ratings with 17770 movies and 480189 users.
Such a huge data set would consume a significant amount of training time and memory
($O(m^2 n)$, since a correlation matrix between users is to be constructed),
which is not possible for our hardware available.
Therefore, only a subset of data is used for evaluation.
To be specific, only first $10000$ movies and first $1000$ users that appear in the data set are considered.

\subsection{Data preprocessing}
The first 1000 movies are loaded into a numpy array with columns of Movie ID, User ID and Rating.
The movie IDs and user IDs are reordered from 0 for the ease of indexing.
Approximately $80\%$ data are then reformatted into a rating matrix $R \in \mathbb R^{m \times n}$ for training,
where $r_{ij}$ is the rating of user $i$ on movie $j$;
the rest are retained for performance evaluation.
The retained and missing data are imputed with the mean rating for the corresponding movie.

\subsection{Hyperparameter selection}
In this KNN model, there are three hyperparameter to be selected, namely
\begin{itemize}
	\item Value of $k$
	\item Similarity metric
	\item Aggregation function
\end{itemize}

\subsubsection{Choice of $k$}
A large value of $k$ would reduce accuracy as users with lower similarity are selected.
On the other hand, a small value of $k$ would be biased over the choice of the most similar user.

As a baseline model, we also set $k=n$, i.e.
to evaluate the RMSE by taking all functions regardless the metric and using only naive arithmeitc mean for aggregation.

% TODO evaluate other choices of k

\subsubsection{Choice of metric}
Since the ratings are discrete in nature, it is expected that little difference is observed between the different metrics.

\subsubsection{Choice of Aggregation function}
Aggregation can be tuned by the similarity metric.
% TODO implement and evaluate other choices of aggregation functions

\subsection{Predictive test set score (RMSE)}
The model is evaluated by computing the RMSE between predicted and actual rating values:
$$ \text{RMSE} = \sqrt{\sum_{(i, j) \in E} \frac{{(\hat r_{ij} - r_{ij})}^2}{\left| E \right|}} $$
where $E$ is the set of retained evaluation data.

\section{Model performance}
The following table exhaustively lists our test results.
The RMSE of the baseline is $1.2062313640217561$.

% \begin{tabular}{|c|c|c|c|}
% 	\hline
% 	$k$ & Similarity metric & Aggregation function & RMSE
% 	\\ \hline
% 	$10$ & Euclidean & Naive arithmetic mean & $1.20094589890527$
% 	\\ \hline
% 	$10$ & Taxicab & Naive arithmetic mean & $1.1948902737623308$
% 	\\ \hline
% 	$10$ & Pearson & Naive arithmetic mean & $1.2001428015368176$
% 	\\ \hline
% 	$10$ & Spearman & Naive arithmetic mean & $1.1996683300387065$
% 	\\ \hline
% \end{tabular}

\begin{tabular}{| c | c | c |}
    \hline
    \multicolumn{3}{|c|}{\textbf{Euclidean}}\\
    \hline
    $k$ & Aggregation function & RMSE\\
    \hline
    $10$ & Naive arithmetic mean & $1.1828937133391502$\\
    \hline
    $10$ & Weighted & $1.2347629060607566$\\
    \hline
\end{tabular}\\

\hspace{2em}

\begin{tabular}{| c | c | c |}
    \hline
    \multicolumn{3}{|c|}{\textbf{Taxicab}}\\
    \hline
    $k$ & Aggregation function & RMSE\\
    \hline
    $10$ & Naive arithmetic mean & $1.1826669333812787$\\
    \hline
    $10$ & Weighted & $1.2359882912097488$\\
    \hline
\end{tabular}\\

\hspace{2em}

\begin{tabular}{| c | c | c |}
    \hline
    \multicolumn{3}{|c|}{\textbf{Pearson}}\\
    \hline
    $k$ & Aggregation function & RMSE\\
    \hline
    $10$ & Naive arithmetic mean & $1.1964910584114306$\\
    \hline
    $10$ & Weighted & $1.2076590696961116$\\
    \hline
\end{tabular}\\

\hspace{2em}

\begin{tabular}{| c | c | c |}
    \hline
    \multicolumn{3}{|c|}{\textbf{Spearman}}\\
    \hline
    $k$ & Aggregation function & RMSE\\
    \hline
    $10$ & Naive arithmetic mean & $1.2084831045336266$\\
    \hline
    $10$ & Weighted & $1.208482338367309$\\
    \hline
\end{tabular}

\hspace{10em}

For the naive arithmetic mean, the Euclidean and Taxicab have a better performance than that of baseline but Pearson and Spearman are not in the case.

For the Weighted method, all metrics did not have better performance than that of baseline. One possible reason for this is related to the lack of data for similar users. When most of the similar users did not have a rating of the movie we are predicting, the formula of weighted average may not include enough information and therefore led to less accurate predictions.

\section{Future work}
We noticed several problems in our design and plan to resolve them in the future weeks.

\subsection{Do not use imputed values for prediction}
Currently, missing data are imputed using the corresponding mean value of ratings on the same movie.
These imputed values do not actually represent opinions of similar users,
and they should not be considered.
It is proposed that we only aggregate real ratings from neighbours,
and only fallback to mean rating if all $K$ neighbours did not rate the movie.

\subsection{Top $Q$ optimization}
Hong and Tsamis proposed only considering the neighbourhood
that intersects with the users with the top $Q < m$ ratings~\cite{Alpher01}.
They suggested that this improves performance as
the distance matrix can be reduced from $O(m^2)$ to $O(Qm)$.
We noticed that this also reduces the chance of the aforementioned problem
where a substantial proportion of the top $K$ neighbours did not the movie
of which a rating is to be predicted.

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}

\end{document}
