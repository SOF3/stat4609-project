% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{csquotes}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\confYear{2021}
%\setcounter{page}{4321} % For final version only

\newcommand{\q}[1]{\enquote{#1}}

\begin{document}

%%%%%%%%% TITLE
\title{
	Assignment 2 - KNN Recommendation System \\~\\
	\large{Team name: M2 Robo}
}

\author{

	Chan Kwan Yin\\
	3035466978 \\
	Team leader
	
	\and

	Lee Chun Yin\\
	3035469140\\
	\and
	Chiu Yu Ying\\
	3035477630

}

\maketitle

\clearpage

\section{Background}

In the KNN model, we assume that similar users will give close ratings on similar movies. When we attempt to predict the rating $\hat{r}$ of target point $(u,v)$, KNN allows us to compute the K most similar points with that target point and average the ratings of them. 

\subsection{Pearson Correlation as a similarity metric}
Considering a user $u_i$ rates movies with a distribution $R_i \sim (\mu_i,\sigma_i)$, the similarity metric between user $u_i$ and the other user $u_j$ would be the correlation coefficient between the two distributions $R_i$ and $R_j :$ $$ \rho_{ij} = \frac{E[(R_i-\mu_i)(R_j-\mu_j)]}{\sigma_i * \sigma_j} $$

To get the k similar data by considering the M movies that both user $u_i$ and $u_j$ have, we estimate the co-variance and variances:
$$ E[(R_i-\mu_i)(R_j-\mu_j)] \approx \frac{1}{M} \sum_k (r_{ik} - \mu_i) (r_{jk} - \mu_j)$$
$$ \sigma_i \approx \sqrt{\frac{1}{M} \sum_k (r_{ik} - \mu_i)} ,
 \sigma_j \approx \sqrt{\frac{1}{M} \sum_k (r_{jk} - \mu_i) ,}$$

It is noticeable that the range of the Pearson Correlation Coefficient is $[-1,1]$ while the matrix in KNN does not contain negative value usually ~\cite{Alpher01}. Therefore, we adopt the formula: $1- \rho_{ij}$ to set the values in the interval of $[0,2]$ which can intuitively visualize the similarity between two users. The smaller the value is, the similar the users are.

\subsection{Spearman Correlation as a similarity metric}

Followed the definition in the Pearson Correlation selction, the Spearman Correlation:

$$ \rho_{{rank}_i,{rank}_j}  = \frac {cov({{rank}_i,{rank}_j})}{\sigma_{rank_i}\sigma_{rank_j}}$$

Spearman Correlation also have the mentioned range issue and we adopt the same formula : $1- \rho_{{rank}_i,{rank}_j}$ to set the values in the desired positive interval.

\subsection{Euclidean Distance as a similarity metric}

The formula of Euclidean Distance between the rating of two users i and j:

$$ d(r_i,r_j) = ((X_ri) - (X_rj)) \cdot ((Y_ri) - (Y_rj)) $$

The range of this metric is $[0,\infty]$ which does not contain negative value. No adjustment is needed.

\subsection{Taxicab as a similarity metric}

The formula of Taxicab between the rating of two users i and j:

$$ |(X_ri) - (X_rj)|+ |(Y_ri) - (Y_rj)| $$

The range of Taxicab also do not contain negative value. No adjustment is needed.

\section{Technical Details}
The training data set provided by Netflix consists of 100 million ratings with 17770 movies and 480189 users. Such a huge data set would consume a significant amount of time waiting and memory during the training and we would have difficulty in amending and testing the changes. There we created a subset of data to be our training set and the testing set. We only use the first 10000 movies for training and evaluation.
For the calculation on correlations, we use the scipy.stats.mstats package to do the computation. For the other metrics (Euclidean Distance, Taxicab), we use the numpy package to set the formula. 

\subsection{Pre-processing}
The first 10000 movies are loaded into a numpy array with columns of Movie ID, User ID, Rating, Year, Day of year and Weekday. The movie IDs and user IDs are reordered from 0 for the ease of indexing.

Then, we reformat the input data into a user-movie matrix and randomly select 20 $\%$ of the available ratings (ratings that are not missing) as test data. The missing data are imputed by the value of the movie mean rating which is the average of the corresponding column of the matrix.

\subsection{Hyperparameter selection}
In this KNN model, there are three hyperparameter to be selected.
\begin{itemize}
	\item Number of K
	\item Metric 
	\item  Weight tuning on average
\end{itemize}

For the number of K, we chose 10 as the value to select 10 points with the highest similarity with the target point for the prediction. The value of K cannot be too large since a huge K value will include those users with less similarity and will give inaccurate results. 

For the metric selection, we chose 4 metrics (Pearson, Spearman,Euclidean Distance, Taxicab) to compare the results given.

For the weight tuning, We suggest that the effect of the user j on user i should correlate with the similarity between user i and user j. The weight of user j should be adjusted based on the rank of the similarity metric calculated.

\subsection{Predictive test set score (RMSE)}

We calculated the deviation between prediction and natural values
$$RMSE = \sqrt{\sum_{i=1}^{N} \frac {{(\hat{r_i}-r_i)}^2}{N}}$$

The RMSE for Pearson Correlation coefficient is 1.2001 and that for Spearman Correlation coefficient is 1.1997. For Euclidean Distance and Taxicab methods, each obtained 1.2009 and 1.1949 respectively. The Taxicab has the best performance with the lowest RMSE value. The following are the Spearman Correlation and the Pearson. The highest RMSE value is obtained by Euclidean Distance metric.

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}

\end{document}